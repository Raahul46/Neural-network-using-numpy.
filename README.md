# Neural network from scratch

1. NN.py
A neural network coded from scratch including the derivates during back propagation that can be used for binary classification.

Activation function: Sigmoid
Output function    : Sigmoid(Binary classification)
Loss function      : Mean squared error
Initialization     : Random 
Weight update      : Vanilla Gradient descent

Architecture
<p>
  <img width="960" height="1280" src="https://github.com/Raahul46/Neural-network-using-numpy./blob/master/Images/WhatsApp%20Image%202020-09-13%20at%208.38.36%20PM.jpeg">
</p>

Mean Squared error
<p>
  <img width="392" height="262" src="https://github.com/Raahul46/Neural-network-using-numpy./blob/master/Images/MSE.png">
</p>

2. FULL NN.py

A full functioning vectorized simple neural network coded from scratch that is used to on the iris dataset(Multi-class classification) to produce a validation accuracy over "96%"

Activation function: Sigmoid, Tanh, Relu and Leaky relu
Output function    : Softmax
Loss function      : Cross entropy
Initialization     : Random, Xavier and He
Weight update      : Vanilla Gradient descent 
Extras             : L2 Normalization



Accuracy 
<p>
  <img width="386" height="262" src="https://github.com/Raahul46/Neural-network-using-numpy./blob/master/Images/Accuracy.png">
</p>

log 
<p>
  <img width="424" height="280" src="https://github.com/Raahul46/Neural-network-using-numpy./blob/master/Images/Log.png">
</p>

snip
<p>
  <img width="391" height="82" src="https://github.com/Raahul46/Neural-network-using-numpy./blob/master/Images/snip.JPG">
</p>


Further work:
Gradient descent     : Momentum based GD, Nestrov accelerated GD, Adagrad, RMSprop and Adam

Strategy             : Mini Batch, Batch and Stochastic 

Extras               : Early stopping, Adding noise filter
